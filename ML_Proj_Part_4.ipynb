{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06356d5c",
   "metadata": {},
   "source": [
    "# Part 4: New Model & Method as a better design for an improved sentiment analysis system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b192922",
   "metadata": {},
   "source": [
    "### Explaining our approach:\n",
    "\n",
    "##### We will opt to use the max-marginal forward-backward algorithm for this part, over the Viterbi algorithm to leverage its ability to provide detailed probabilistic insights into each state's likelihood at every time step, rather than merely identifying the most likely overall sequence of states. \n",
    "\n",
    "#####  While both algorithms are probabilistic, the Viterbi algorithm provides a deterministic output (the single best sequence), based on probabilistic calculations. In contrast, the max-marginal forward-backward algorithm offers a broader probabilistic perspective by detailing the likelihood of each possible state at every point in the sequence, making it more suitable for applications where such detailed probabilistic information is valuable. Another key thing to note is, that the scores in the max_marginal algotrithm, actually calculateds the probability of each state, independently of the path decisions, giving a more comprehensive view of state probabilities considering all poossible paths, not just the most likely path. In the case of this project, speech tagging, the max-marginal forward backward decoding is better as with its probabilistic information, it can generally help with handling ambiguity and model confidence making it \"better\" than the Viterbi Algorithm.\n",
    "\n",
    "##### To address the potential concern, that this max_marginal forward-backward algorithm might not yield better results than the Viterbi implementation we had earlier on, we couple our max_marginal forward-backward algorithm, with a new and better way to handlw unknown words, which will give us more accurate emission parameters. In this way, we will be able to yield a more accurate sequence of tags compared to part 3. And this would also have the added benefit of being able to indicate how certain we are about each state, which can be very useful sometimes.\n",
    "\n",
    "##### We also would like to take this chance to try implementing a model other than the Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3abf86",
   "metadata": {},
   "source": [
    "### New model used: Max-Marginal Forward-Backward\n",
    "\n",
    "##### We make use of the max-marginal decoding algorithm that employs the forward-backward method to calculate individual state probabilities at each time step in a sequence, by computing forward (α) and backward (β) probabilities. It selects the most likely state for each position by identifying the state that maximizes the product of these probabilities (αu(i) * βu(i)). \n",
    "\n",
    "### New method to smooth emission parameters: Absolute Discounting\n",
    "\n",
    "##### Absolute discounting is a smoothing technique used in language modeling to address data sparsity issues by adjusting the counts of observed events. It works by subtracting a fixed discount factor from the counts, ensuring that no count becomes negative. This redistribution of probability mass from observed to unseen events helps the model generalize better, particularly in the presence of rare or unseen words. By allocating a portion of probability mass to unseen events, absolute discounting improves the model's ability to make accurate predictions, even in scenarios where training data is limited or incomplete. Overall, absolute discounting enhances the robustness and performance of language models by providing more reliable probability estimates and mitigating the impact of data sparsity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca1cb5",
   "metadata": {},
   "source": [
    "### Processing The File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48c08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath):\n",
    "    # we make use of the default library \"collections\" to make processing the tags and word-tag pairs easier\n",
    "    import collections #used for counting\n",
    "    tag_count = collections.defaultdict(int)  # counting for tags\n",
    "    word_tag_count = collections.defaultdict(int)  # counting for word-tag pairs\n",
    "    vocabulary = set()  # stores unique words\n",
    "    sentences = [] # store all the sentences\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        # reading file line-by-line\n",
    "        for line in file:\n",
    "            stripped_line = line.strip() #removes the /n and then splits it to separete the word and its label\n",
    "            if stripped_line:  # check if there even is a word or tag in the line\n",
    "                word, tag = stripped_line.split()  # Split line into word and tag\n",
    "                word_tag_count[(word, tag)] += 1\n",
    "                tag_count[tag] += 1\n",
    "                vocabulary.add(word) #doesnt add duplicates\n",
    "                current_sentence.append(word)\n",
    "            else:\n",
    "                if current_sentence: \n",
    "                    # add current sentence to sentences then restart the count\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return tag_count, word_tag_count, vocabulary, sentences\n",
    "\n",
    "\n",
    "#tag count : dictionary with the count of each tag e.g ('B-NP') : 45\n",
    "#word_tag_count : dictionary with the count of each word-tag pair e.g ('Municipal','B-NP') : 1\n",
    "\n",
    "tag_count, word_tag_count, vocabulary, sentences = process_file('EN/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab235858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_for_transitions(filepath):\n",
    "    # we make use of the default library \"collections\" to make processing the tags and word-tag pairs easier\n",
    "    import collections\n",
    "    transition_count = collections.defaultdict(int) #y_u to y_v, including start and stop\n",
    "    tag_count = collections.defaultdict(int)  # counting for tags\n",
    "    vocab = set()\n",
    "    \n",
    "    # we still need counters for stop and start, to add them into the transition parameters\n",
    "    start_counter = 0 \n",
    "    stop_counter = 0\n",
    "    \n",
    "    START = \"START\"\n",
    "    STOP = \"STOP\"\n",
    "    previous_tag = START\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line:\n",
    "                word, tag = stripped_line.split()\n",
    "                transition_count[(previous_tag, tag)] += 1\n",
    "                if previous_tag == \"START\":\n",
    "                    start_counter += 1\n",
    "                tag_count[tag] += 1\n",
    "                previous_tag = tag\n",
    "                vocab.add(word)\n",
    "            else:  # when the sentence has ended\n",
    "                transition_count[(previous_tag, STOP)] += 1\n",
    "                stop_counter += 1\n",
    "                previous_tag = START  # reset for the next sentence\n",
    "     #adding counts for start and stop\n",
    "    tag_count[\"START\"] = start_counter\n",
    "    tag_count[\"STOP\"] = stop_counter\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    # split on double newlines which denote separated sentences in our case\n",
    "    sentences = [sentence.split() for sentence in content.split('\\n\\n')]\n",
    "\n",
    "    return transition_count, tag_count, sentences, vocab\n",
    "\n",
    "def estimate_all_transition_probability(transition_count, tag_count):\n",
    "  \n",
    "    transition_probabilities = {}\n",
    "    # iterate through all the transition tag pairs to get all the transition probabilities\n",
    "    # store the results in the dictionary transition_probabilities\n",
    "    for (y_u, y_v), count in transition_count.items():\n",
    "        transition_probabilities[(y_u, y_v)] = count / tag_count[y_u]\n",
    "        \n",
    "    return transition_probabilities\n",
    "\n",
    "\n",
    "# run the function to get all the transiiton probaibilities\n",
    "transition_count, tag_count, sentences, vocabulary = process_file_for_transitions('EN/train')\n",
    "transition_probabilities = estimate_all_transition_probability(transition_count, tag_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53bd4b",
   "metadata": {},
   "source": [
    "### Function to write outputs to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe98e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    # split on double newlines which denote separated sentences in our case\n",
    "    return [sentence.split() for sentence in content.split('\\n\\n')]\n",
    "\n",
    "def get_prediction(filepath, tag_count, transmission_probabilities, emission_probabilities, vocabulary):\n",
    "    sentences = get_sentences(filepath)\n",
    "#     print(sentences)\n",
    "    predictions = []\n",
    "    for sentence in sentences:\n",
    "        best_path_prediction = forward_backward_decoding(sentence, tag_count, transmission_probabilities, emission_probabilities, vocabulary)\n",
    "        predictions.append(list(zip(sentence, best_path_prediction))) #puts them in the predictions array pairwise\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b97f32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tag_predictions_to_file(predictions, output_filepath):\n",
    "    # open the output file for writing\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "        for sentence in predictions:\n",
    "            for word, tag in sentence:\n",
    "                # write each word and its predicted tag to the file, with a spacing to separate.\n",
    "                file.write(f\"{word} {tag}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2532e",
   "metadata": {},
   "source": [
    "### 4a, 15 points (New method and New model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ea246ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_probabilities_absolute_discounting(tag_count, word_tag_count, d=0.97):\n",
    "    emission_probabilities = {}\n",
    "    total_unique_words = sum(tag_count.values())\n",
    "\n",
    "    for (word, tag), count in word_tag_count.items():\n",
    "        adjusted_count = max(count - d, 0)\n",
    "        emission_probabilities[(word, tag)] = adjusted_count / tag_count[tag]\n",
    "\n",
    "    # Handle unseen words by assigning the discounted probability mass\n",
    "    for tag in tag_count:\n",
    "        unseen_prob = d * len([word for word, t in word_tag_count if t == tag]) / total_unique_words\n",
    "        emission_probabilities[(\"#UNK#\", tag)] = unseen_prob\n",
    "\n",
    "    return emission_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2df9592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function to get emission parameters\n",
    "new_emission_probabilities = estimate_emission_probabilities_absolute_discounting(tag_count, word_tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f51804be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_decoding(sentence, tag_count, transition_probabilities, new_emission_probabilities, vocabulary):\n",
    "    tags = [tag for tag in tag_count if tag not in ['START', 'STOP']]  # makes a dictionary of tags that doesnt include start and stop, so we dont iterate through them unncessarily\n",
    "    n = len(sentence)\n",
    "    m = len(tags)\n",
    "    \n",
    "   #replace all unknown words with the special token #UNK# so that we can properly handle the emission probabilities\n",
    "    # we want to directly alter the sentence to have '#UNK#' as that's the requirement from Part 1\n",
    "    for i in range(0,n):\n",
    "        if sentence[i] not in vocabulary:\n",
    "            sentence[i] = '#UNK#'\n",
    "    \n",
    "    # initialize the arrays to store the forward and backward scores\n",
    "    alpha = [[float(0) for _ in range(m)] for _ in range(n)]\n",
    "    beta = [[float(0) for _ in range(m)] for _ in range(n)]\n",
    "    \n",
    "    # Base case for forward probabilities\n",
    "    for i, tag in enumerate(tags):\n",
    "        alpha[0][i] = transition_probabilities.get(('START', tag), 0) * new_emission_probabilities.get((sentence[0], tag), 0)\n",
    "\n",
    "    # Bottom up dynamimc programming to calculate forward probabilities\n",
    "    for t in range(1, n):\n",
    "        for j, tag in enumerate(tags):\n",
    "            sum_alpha = 0\n",
    "            for i, prev_tag in enumerate(tags):\n",
    "                sum_alpha += alpha[t-1][i] * transition_probabilities.get((prev_tag, tag), 0) * new_emission_probabilities.get((sentence[t], tag), 0)\n",
    "            alpha[t][j] = sum_alpha\n",
    "\n",
    "    # Base case for backward probabilities\n",
    "    for i, tag in enumerate(tags):\n",
    "        beta[n-1][i] = transition_probabilities.get((tag, 'STOP'), 0) * new_emission_probabilities.get((sentence[t], tag), 0)\n",
    "\n",
    "    # Bottom up dynamimc programming to calculate backward probabilities\n",
    "    for t in range(n-2, -1, -1):\n",
    "        for i, tag in enumerate(tags):\n",
    "            sum_beta = 0\n",
    "            for j, next_tag in enumerate(tags):\n",
    "                sum_beta += beta[t+1][j] * transition_probabilities.get((tag, next_tag), 0) * new_emission_probabilities.get((sentence[t+1], next_tag), 0)\n",
    "            beta[t][i] = sum_beta\n",
    "\n",
    "    # Determine the best tags by finding the maximum alpha * beta product for each position\n",
    "    # essentially we are looking for argmax alpha * beta at iter u\n",
    "    best_tags = []\n",
    "    for t in range(n):\n",
    "        # initialise max score nad best tag\n",
    "        max_score = -1\n",
    "        best_tag = None\n",
    "        for i, tag in enumerate(tags):\n",
    "            score = alpha[t][i] * beta[t][i]\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_tag = tag\n",
    "        best_tags.append(best_tag)\n",
    "\n",
    "    return best_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "731b7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = get_prediction('EN/dev.in', tag_count, transition_probabilities, new_emission_probabilities, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1432bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tag_predictions_to_file(prediction, 'EN/dev.p4.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "863999cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 14816\n",
      "\n",
      "#Correct Entity : 10691\n",
      "Entity  precision: 0.7216\n",
      "Entity  recall: 0.8112\n",
      "Entity  F: 0.7638\n",
      "\n",
      "#Correct Sentiment : 9734\n",
      "Sentiment  precision: 0.6570\n",
      "Sentiment  recall: 0.7386\n",
      "Sentiment  F: 0.6954\n"
     ]
    }
   ],
   "source": [
    "# evaluate the scores of the max marginal forward-backward algorithm implementation\n",
    "!python3 EvalScript/evalResult.py EN/dev.out EN/dev.p4.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bd060",
   "metadata": {},
   "source": [
    "##### We can see here that actually, this implementation of max-marginal decoding does not perform as well as the viterbi algorithm that we implemented in part 3 by a little bit. Hence, we are actually sacrificing some accuracy, but this method is still better due to its probabilistic advantages mentioned above. Overall, the trade off is better in some scenarios so we are confident in this method that we have chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80a66c",
   "metadata": {},
   "source": [
    "### 4b, 10 points (Evaluation using a new test set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b7a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
