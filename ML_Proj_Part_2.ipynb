{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4930f3",
   "metadata": {},
   "source": [
    "# Part 2: Estimate Transition Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aca745",
   "metadata": {},
   "source": [
    "### Code from Part 1 Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "946bec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath):\n",
    "    # we make use of the default library \"collections\" to make processing the tags and word-tag pairs easier\n",
    "    import collections #used for counting\n",
    "    tag_count = collections.defaultdict(int)  # counting for tags\n",
    "    word_tag_count = collections.defaultdict(int)  # counting for word-tag pairs\n",
    "    vocabulary = set()  # stores unique words\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        # reading file line-by-line\n",
    "        for line in file:\n",
    "            stripped_line = line.strip() #removes the /n and then splits it to separete the word and its label\n",
    "            if stripped_line:  # check if there even is a word or tag in the line\n",
    "                word, tag = stripped_line.split()  # Split line into word and tag\n",
    "                word_tag_count[(word, tag)] += 1\n",
    "                tag_count[tag] += 1\n",
    "                vocabulary.add(word) #doesnt add duplicates\n",
    "                current_sentence.append(word)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return tag_count, word_tag_count, vocabulary, sentences\n",
    "\n",
    "\n",
    "#tag count : dictionary with the count of each tag e.g ('B-NP') : 45\n",
    "#word_tag_count : dictionary with the count of each word-tag pair e.g ('Municipal','B-NP') : 1\n",
    "\n",
    "tag_count, word_tag_count, vocabulary, sentences = process_file('EN/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c8de2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_all_emission_probabilities_with_unknown(tag_count, word_tag_count, k =0.1):\n",
    "  \n",
    "    emission_probabilities = {}\n",
    "    # iterate through all the word tag pairs to get all the emission probabilities\n",
    "    # store the results in the dictionary emission_probabilities\n",
    "    for (word, tag), count in word_tag_count.items():\n",
    "        \n",
    "        emission_probabilities[(word, tag)] = count / (tag_count[tag]+k)\n",
    "        \n",
    "    for tag, count in tag_count.items():\n",
    "        emission_probabilities[(\"#UNK#\", tag)] = count / (tag_count[tag]+k)\n",
    "    return emission_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42080c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probabilities = estimate_all_emission_probabilities_with_unknown(tag_count, word_tag_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcd2f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_for_transitions(filepath):\n",
    "    # we make use of the default library \"collections\" to make processing the tags and word-tag pairs easier\n",
    "    import collections\n",
    "    transition_count = collections.defaultdict(int) #y_u to y_v, including start and stop\n",
    "    tag_count = collections.defaultdict(int)  # counting for tags\n",
    "    vocab = set()\n",
    "    \n",
    "    # we still need counters for stop and start, to add them into the transition parameters\n",
    "    start_counter = 0 \n",
    "    stop_counter = 0\n",
    "    \n",
    "    START = \"START\"\n",
    "    STOP = \"STOP\"\n",
    "    previous_tag = START\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line:\n",
    "                word, tag = stripped_line.split()\n",
    "                transition_count[(previous_tag, tag)] += 1\n",
    "                if previous_tag == \"START\":\n",
    "                    start_counter += 1\n",
    "                tag_count[tag] += 1\n",
    "                previous_tag = tag\n",
    "                vocab.add(word)\n",
    "            else:  # when the sentence has ended\n",
    "                transition_count[(previous_tag, STOP)] += 1\n",
    "                stop_counter += 1\n",
    "                previous_tag = START  # reset for the next sentence\n",
    "     #adding counts for start and stop\n",
    "    tag_count[\"START\"] = start_counter\n",
    "    tag_count[\"STOP\"] = stop_counter\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    # split on double newlines which denote separated sentences in our case\n",
    "    sentences = [sentence.split() for sentence in content.split('\\n\\n')]\n",
    "\n",
    "    return transition_count, tag_count, sentences, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8416040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    # split on double newlines which denote separated sentences in our case\n",
    "    return [sentence.split() for sentence in content.split('\\n\\n')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1006d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the functions \n",
    "transition_count, tag_count, sentences, vocabulary = process_file_for_transitions('EN/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753d2ff",
   "metadata": {},
   "source": [
    "## Functions to write to output file\n",
    "\n",
    "##### We make use of these functions to write the predictions to an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79f73380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to use viterbi to predict sentence by sentence\n",
    "def get_prediction(filepath, tag_count, transition_probabilities, emission_probabilities, vocabulary):\n",
    "    sentences = get_sentences(filepath)\n",
    "    predictions = [] #initialise the list of sentences\n",
    "    for sentence in sentences:\n",
    "       \n",
    "        # predict the best path, sentence by sentence\n",
    "        best_path_prediction = viterbi_algorithm(sentence, tag_count, transition_probabilities, emission_probabilities, vocabulary)\n",
    "       \n",
    "        #puts the word - predicted tag pairs in the predictions array pairwise\n",
    "        predictions.append(list(zip(sentence, best_path_prediction))) \n",
    "        \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "361b19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tag_predictions_to_file(prediction, output_filepath):\n",
    "    # open the output file for writing\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "        for sentence in predictions:\n",
    "            for word, tag in sentence:\n",
    "                # write each word and its predicted tag to the file, with a spacing to separate them.\n",
    "                file.write(f\"{word} {tag}\\n\")\n",
    "            # leave an empty line between sentences\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc699f1",
   "metadata": {},
   "source": [
    "## Part 2a, 10 points (estimate transition parameters)\n",
    "###### we have a function to calculate the transitiuon probability of a set of u,v states, based on the tag and transition counts. We also have another function to calculate the transition probability of all the different transitions present in the file.\n",
    "\n",
    "###### we will be using the latter mainly for this projject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b120ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_one_transition_probability(y_u, y_v, transition_count, tag_count):\n",
    "        # get the total times y->x occurs\n",
    "    tag_transition_freq = transition_count.get((y_u, y_v), 0)\n",
    "    # total times y appears\n",
    "    tag_total_freq = tag_count.get(y, 1)\n",
    "    \n",
    "    return tag_transition_freq / tag_total_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7457b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_all_transition_probability(transition_count, tag_count):\n",
    "  \n",
    "    transition_probabilities = {}\n",
    "    # iterate through all the transition tag pairs to get all the transition probabilities\n",
    "    # store the results in the dictionary transition_probabilities\n",
    "    for (y_u, y_v), count in transition_count.items():\n",
    "        transition_probabilities[(y_u, y_v)] = count / tag_count[y_u]\n",
    "        \n",
    "    return transition_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5456774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function to get all the transiiton probaibilities\n",
    "transition_probabilities = estimate_all_transition_probability(transition_count, tag_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08645280",
   "metadata": {},
   "source": [
    "## Part 2b, 15 points (viterbi algorithm implementation)\n",
    "\n",
    "###### We have a function that runs the Viterbi algorithm to obtain the ideal sequence of tags for a given sequence of observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "651857b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence, tag_count, transition_probabilities, emission_probabilities, vocabulary, unk = 0.1):\n",
    "    # make sure that \"sentence\" is a sequence of x observations\n",
    "    \n",
    "    tags = [tag for tag in tag_count if tag not in ['START', 'STOP']] \n",
    "    # makes a dictionary of tags that doesnt include start and stop, so we dont iterate through them unncessarily except \n",
    "    # at the actual start and end of the sentence\n",
    "    \n",
    "    n = len(sentence)  # number of words in the sentence (k)\n",
    "    m = len(tags)      # number of tags (u / v)\n",
    "    \n",
    "    # to account for unknown words. For this to work effectively, the emission parameters should include the probabilities for unknown words.\n",
    "    for i in range(0,n):\n",
    "        if sentence[i] not in vocabulary:\n",
    "            sentence[i] = '#UNK#'\n",
    "    \n",
    "    # create a matrix to store all the pi values, initialised at \"-inf\" so we can easily look for max score or probability to input\n",
    "    pi = [[float('-inf')] * m for _ in range(n+1)] #+1 tp account for the stop state, but we dont actually store anything theer\n",
    "    backpointer = [[0] * m for _ in range(n)] # to store y*\n",
    "\n",
    "    # we skip the step of assigning pi(0,v) = 1 if v is start and 0 otherwise, so our base case is when u is \"START\"\n",
    "    for i, tag in enumerate(tags): # i is the index of the tag\n",
    "        t_count = tag_count[tag]\n",
    "        pi[0][i] = transition_probabilities.get(('START', tag), 0) * emission_probabilities.get((sentence[0], tag),0)\n",
    "\n",
    "            \n",
    "    #bottom up dynamic programming, updating the next values of pi based on the previous values of pi\n",
    "    for i in range(1, n): \n",
    "        for j, tag in enumerate(tags):\n",
    "            max_prob = float('-inf')\n",
    "            max_state = None\n",
    "            for kk, prev_tag in enumerate(tags):\n",
    "                prob = pi[i-1][kk] * transition_probabilities.get((prev_tag, tag), 0) * emission_probabilities.get((sentence[i], tag), 0)\n",
    "                \n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_state = kk\n",
    "                    \n",
    "            pi[i][j] = max_prob\n",
    "            backpointer[i][j] = max_state # to store y*\n",
    "\n",
    "    # termination step \n",
    "    max_prob = float('-inf')\n",
    "    max_state = None\n",
    "    for i, tag in enumerate(tags):\n",
    "        prob = pi[n-1][i] * transition_probabilities.get((tag, 'STOP'), 0)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_state = i\n",
    "            # no need to store in backpointer\n",
    "            \n",
    "    #initialise an array for the best sequence of states using the max state from the termination step    \n",
    "    best_path = [tags[max_state]]\n",
    "    \n",
    "    #go backwards along the backpointer, iteratively finding the best state\n",
    "    for i in range(n-1, 0, -1):\n",
    "        max_state = backpointer[i][max_state]\n",
    "        best_path.append(tags[max_state])\n",
    "        \n",
    "    # reverse the array to get the sequence of states or tags for the observations in the right order\n",
    "    best_path.reverse()\n",
    "    \n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29a6e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the functions to write predictions to file\n",
    "predictions = get_prediction('EN/dev.in', tag_count, transition_probabilities, emission_probabilities, vocabulary)\n",
    "write_tag_predictions_to_file(predictions, 'EN/dev.p2.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d414c3ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 14279\n",
      "\n",
      "#Correct Entity : 10858\n",
      "Entity  precision: 0.7604\n",
      "Entity  recall: 0.8239\n",
      "Entity  F: 0.7909\n",
      "\n",
      "#Correct Sentiment : 10056\n",
      "Sentiment  precision: 0.7043\n",
      "Sentiment  recall: 0.7630\n",
      "Sentiment  F: 0.7325\n"
     ]
    }
   ],
   "source": [
    "# evaluate the scores of the viterbi algorithm implementation\n",
    "!python3 EvalScript/evalResult.py EN/dev.out EN/dev.p2.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
