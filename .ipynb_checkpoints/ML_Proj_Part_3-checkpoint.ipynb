{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47c81ea",
   "metadata": {},
   "source": [
    "## Part 3, 25 points (find second best sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a29844",
   "metadata": {},
   "source": [
    "### Required helper functions to get all the parameters needed for our modified viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f569f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath):\n",
    "    # we make use of the default library \"collections\" to make processing the tags and word-tag pairs easier\n",
    "    import collections #used for counting\n",
    "    tag_count = collections.defaultdict(int)  # counting for tags\n",
    "    word_tag_count = collections.defaultdict(int)  # counting for word-tag pairs\n",
    "    vocabulary = set()  # stores unique words\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        # reading file line-by-line\n",
    "        for line in file:\n",
    "            stripped_line = line.strip() #removes the /n and then splits it to separete the word and its label\n",
    "            if stripped_line:  # check if there even is a word or tag in the line\n",
    "                word, tag = stripped_line.split()  # Split line into word and tag\n",
    "                word_tag_count[(word, tag)] += 1\n",
    "                tag_count[tag] += 1\n",
    "                vocabulary.add(word) #doesnt add duplicates\n",
    "                current_sentence.append(word)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return tag_count, word_tag_count, vocabulary, sentences\n",
    "\n",
    "\n",
    "#tag count : dictionary with the count of each tag e.g ('B-NP') : 45\n",
    "#word_tag_count : dictionary with the count of each word-tag pair e.g ('Municipal','B-NP') : 1\n",
    "\n",
    "tag_count, word_tag_count, vocabulary, sentences = process_file('EN/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e6e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_for_transitions(filepath):\n",
    "    # we make use of the default library \"collections\" to make processing the tags and word-tag pairs easier\n",
    "    import collections\n",
    "    transition_count = collections.defaultdict(int) #y_u to y_v, including start and stop\n",
    "    tag_count = collections.defaultdict(int)  # counting for tags\n",
    "    vocab = set()\n",
    "    \n",
    "    # we still need counters for stop and start, to add them into the transition parameters\n",
    "    start_counter = 0 \n",
    "    stop_counter = 0\n",
    "    \n",
    "    START = \"START\"\n",
    "    STOP = \"STOP\"\n",
    "    previous_tag = START\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line:\n",
    "                word, tag = stripped_line.split()\n",
    "                transition_count[(previous_tag, tag)] += 1\n",
    "                if previous_tag == \"START\":\n",
    "                    start_counter += 1\n",
    "                tag_count[tag] += 1\n",
    "                previous_tag = tag\n",
    "                vocab.add(word)\n",
    "            else:  # when the sentence has ended\n",
    "                transition_count[(previous_tag, STOP)] += 1\n",
    "                stop_counter += 1\n",
    "                previous_tag = START  # reset for the next sentence\n",
    "     #adding counts for start and stop\n",
    "    tag_count[\"START\"] = start_counter\n",
    "    tag_count[\"STOP\"] = stop_counter\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    # split on double newlines which denote separated sentences in our case\n",
    "    sentences = [sentence.split() for sentence in content.split('\\n\\n')]\n",
    "\n",
    "    return transition_count, tag_count, sentences, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b026e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    # split on double newlines which denote separated sentences in our case\n",
    "    return [sentence.split() for sentence in content.split('\\n\\n')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a2eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the functions \n",
    "transition_count, tag_count, sentences, vocabulary = process_file_for_transitions('EN/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dbe2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to use viterbi to predict sentence by sentence\n",
    "def get_prediction(filepath, tag_count, transition_probabilities, emission_probabilities, vocabulary):\n",
    "    sentences = get_sentences(filepath)\n",
    "    predictions = [] #initialise the list of sentences\n",
    "    for sentence in sentences:\n",
    "       \n",
    "        # predict the best path, sentence by sentence\n",
    "        best_path_prediction = modified_viterbi_algorithm(sentence, tag_count, transition_probabilities, emission_probabilities, vocabulary)\n",
    "       \n",
    "        #puts the word - predicted tag pairs in the predictions array pairwise\n",
    "        predictions.append(list(zip(sentence, best_path_prediction))) \n",
    "        \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "787e0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tag_predictions_to_file(predictions, output_filepath):\n",
    "    # open the output file for writing\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "        for sentence in predictions:\n",
    "            for word, tag in sentence:\n",
    "                # write each word and its predicted tag to the file, with a spacing to separate them.\n",
    "                file.write(f\"{word} {tag}\\n\")\n",
    "            # leave an empty line between sentences\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7176d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_all_emission_probabilities_with_unknown(tag_count, word_tag_count, k =0.1):\n",
    "  \n",
    "    emission_probabilities = {}\n",
    "    # iterate through all the word tag pairs to get all the emission probabilities\n",
    "    # store the results in the dictionary emission_probabilities\n",
    "    for (word, tag), count in word_tag_count.items():\n",
    "        \n",
    "        emission_probabilities[(word, tag)] = count / (tag_count[tag]+k)\n",
    "        \n",
    "    for tag, count in tag_count.items():\n",
    "        emission_probabilities[(\"#UNK#\", tag)] = count / (tag_count[tag]+k)\n",
    "    return emission_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8d3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probabilities = estimate_all_emission_probabilities_with_unknown(tag_count, word_tag_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6729af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_all_transition_probability(transition_count, tag_count):\n",
    "  \n",
    "    transition_probabilities = {}\n",
    "    # iterate through all the transition tag pairs to get all the transition probabilities\n",
    "    # store the results in the dictionary transition_probabilities\n",
    "    for (y_u, y_v), count in transition_count.items():\n",
    "        transition_probabilities[(y_u, y_v)] = count / tag_count[y_u]\n",
    "        \n",
    "    return transition_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63d6f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function to get all the transiiton probaibilities\n",
    "transition_probabilities = estimate_all_transition_probability(transition_count, tag_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5aed0d",
   "metadata": {},
   "source": [
    "## Our approach to modifying the viterbi algorithm to obtain the second best path includes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6eba1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 24725\n",
      "\n",
      "#Correct Entity : 7231\n",
      "Entity  precision: 0.2925\n",
      "Entity  recall: 0.5487\n",
      "Entity  F: 0.3815\n",
      "\n",
      "#Correct Sentiment : 2114\n",
      "Sentiment  precision: 0.0855\n",
      "Sentiment  recall: 0.1604\n",
      "Sentiment  F: 0.1115\n"
     ]
    }
   ],
   "source": [
    "def modified_viterbi_algorithm(sentence, tag_count, transmission_probabilities, emission_probabilities, vocabulary, unk = 0.1):\n",
    "    # make sure that \"sentence\" is a sequence of x observations\n",
    "    tags = [tag for tag in tag_count if tag not in ['START', 'STOP']] # makes a dictionary of tags that doesnt include start and stop, so we dont iterate through them unncessarily\n",
    "#     tags.pop('START')\n",
    "#     tags.pop('STOP')\n",
    "    \n",
    "    n = len(sentence)  # number of words in the sentence (k)\n",
    "    m = len(tags)      # number of tags (u / v)\n",
    "    for i in range(0,n):\n",
    "        if sentence[i] not in vocabulary:\n",
    "            sentence[i] = '#UNK#'\n",
    "    # create a matrix to store all the pi values\n",
    "    pi = [[float('-inf')] * m for _ in range(n+1)] #+1 tp account for the stop state, but we dont actually store anything theer\n",
    "    backpointer = [[[0,0]] * m for _ in range(n)] # to store y*\n",
    "\n",
    "    # base case!!!, here, we actually just initialise the first column to be pi(0,v) where u is \"START\". \n",
    "    # we skip the step of assigning pi(0,v) = 1 if v is start and 0 otherwise\n",
    "    for i, tag in enumerate(tags): # i is the index of the tag\n",
    "        t_count = tag_count[tag]\n",
    "        pi[0][i] = transmission_probabilities.get(('START', tag), 0) * emission_probabilities.get((sentence[0], tag),0)\n",
    "\n",
    "            \n",
    "    #bottom up dynamic programming        \n",
    "    for i in range(1, n): \n",
    "        for j, tag in enumerate(tags):\n",
    "            max_prob = float('-inf')\n",
    "            max_state = None\n",
    "            prev_max_state = None\n",
    "\n",
    "            for kk, prev_tag in enumerate(tags):\n",
    "                # for now, if there's an unknown word, default to 0, but we should change this to the k/count + k thing\n",
    "#                 t_count\n",
    "                \n",
    "\n",
    "                prob = pi[i-1][kk] * transmission_probabilities.get((prev_tag, tag), 0) * emission_probabilities.get((sentence[i], tag), 0)\n",
    "                # We are going through every tag that could have existed at the previous state, and finding the maximum probability\n",
    "                # of transitioning from that tag to the current tag, and multiplying it by the emission probability of the current word\n",
    "\n",
    "                # if the probability is greater than the max probability, update the max probability and the max state\n",
    "                # max state is the tag that gave us the maximum probability of transitioning to the current tag\n",
    "                # max probability is the maximum probability of transitioning to the current tag\n",
    "                \n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    prev_max_state = max_state\n",
    "                    max_state = kk\n",
    "\n",
    "            pi[i][j] = max_prob\n",
    "            backpointer[i][j][0] = max_state # to store y*\n",
    "            backpointer[i][j][1] = prev_max_state\n",
    "\n",
    "            #backpointer contains the best state that led to the current state\n",
    "            #backpointer[i][j] = k means that the state that led to the current state is k\n",
    "\n",
    "    # termination step \n",
    "    max_prob = float('-inf')\n",
    "    max_state = None\n",
    "    # we want to find the maximum probability of transitioning from the last word to the stop state\n",
    "\n",
    "\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        prob = pi[n-1][i] * transmission_probabilities.get((tag, 'STOP'), 0) # we dont need to multiply by the emission probability of the stop state because it is always 1\n",
    "        # We are going through every tag at n-1 time stamp (time stamp before 'STOP')\n",
    "        # To see which state has the best chance to lead to a STOP state\n",
    "\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob # Probability of the max state leading to stop\n",
    "            max_state = i # Most probable state to lead to stop\n",
    "    \n",
    "    \n",
    "            \n",
    "    #initialise an array for the best sequence of states         \n",
    "\n",
    "    second_best = [tags[max_state]]\n",
    "\n",
    "    second_best_explored = False\n",
    "    #go backwards along the backpointer, iteratively finding the best state\n",
    "    for i in range(n-1, 0, -1): # n is going down by -1, we're going down time stamps from n-1 to 1\n",
    "\n",
    "        max_state = backpointer[i][max_state][0] # get the best state that led to the current state\n",
    "\n",
    "\n",
    "        if backpointer[i][max_state][1] != None and second_best_explored == False: # next best path exists and that we havent explored it yet\n",
    "            second_best.append(tags[backpointer[i][max_state][1]])\n",
    "\n",
    "            second_best_explored = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            second_best.append(tags[backpointer[i][max_state][0]])\n",
    "\n",
    "    # reverse the array to get it in the right order\n",
    "    second_best.reverse()\n",
    "    \n",
    "    return second_best\n",
    "\n",
    "predictions= get_prediction('EN/dev.in', tag_count, transition_probabilities, emission_probabilities, vocabulary)\n",
    "# print(predictions)\n",
    "\n",
    "write_tag_predictions_to_file(predictions, 'EN/dev.p3.out')\n",
    "\n",
    "!python3 EvalScript/evalResult.py EN/dev.out EN/dev.p3.out\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
